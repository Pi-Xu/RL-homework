{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qUmV93fif6S"
   },
   "source": [
    "## Run Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "lN-gZkqiijnR"
   },
   "outputs": [],
   "source": [
    "#@title imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
    "from cs285.agents.pg_agent import PGAgent\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "Q6NaOWhOinnU"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  def __contains__(self, key):\n",
    "    return hasattr(self, key)\n",
    "\n",
    "  env_name = 'LunarLanderContinuous-v2' #@param\n",
    "  exp_name = 'q3_b40000_r0.005' #@param\n",
    "\n",
    "  #@markdown main parameters of interest\n",
    "  n_iter = 100 #@param {type: \"integer\"}\n",
    "\n",
    "  ## PDF will tell you how to set ep_len\n",
    "  ## and discount for each environment\n",
    "  ep_len = 200 #@param {type: \"integer\"}\n",
    "  discount = 0.99 #@param {type: \"number\"}\n",
    "\n",
    "  reward_to_go = True #@param {type: \"boolean\"}\n",
    "  nn_baseline = True #@param {type: \"boolean\"}\n",
    "  gae_lambda = None #@param {type: \"number\"}\n",
    "  dont_standardize_advantages = False #@param {type: \"boolean\"}\n",
    "\n",
    "  #@markdown batches and steps\n",
    "  batch_size = 40000 #@param {type: \"integer\"}\n",
    "  eval_batch_size = 400 #@param {type: \"integer\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
    "  learning_rate =  5e-3 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown MLP parameters\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown system\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "    \n",
    "  action_noise_std = 0 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  ## default is to not log video so\n",
    "  ## that logs are small enough to be\n",
    "  ## uploaded to gradscope\n",
    "  video_log_freq =  -1#@param {type: \"integer\"}\n",
    "  scalar_log_freq =  1#@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "## ensure compatibility with hw1 code\n",
    "args['train_batch_size'] = args['batch_size']\n",
    "\n",
    "if args['video_log_freq'] > 0:\n",
    "  import warnings\n",
    "  warnings.warn(\n",
    "      '''\\nLogging videos will make eventfiles too'''\n",
    "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
    "      '''\\nfor the runs you intend to submit.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "eScWwHhnsYkd"
   },
   "outputs": [],
   "source": [
    "#@title create directory for logging\n",
    "\n",
    "data_path =r'D:\\Code\\RL-homework\\hw2\\data'\n",
    "\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "logdir = args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aljzrLdAsvNu"
   },
   "outputs": [],
   "source": [
    "## define policy gradient trainer\n",
    "\n",
    "class PG_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "\n",
    "        #####################\n",
    "        ## SET AGENT PARAMS\n",
    "        #####################\n",
    "\n",
    "        computation_graph_args = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            }\n",
    "\n",
    "        estimate_advantage_args = {\n",
    "            'gamma': params['discount'],\n",
    "            'standardize_advantages': not(params['dont_standardize_advantages']),\n",
    "            'reward_to_go': params['reward_to_go'],\n",
    "            'nn_baseline': params['nn_baseline'],\n",
    "            'gae_lambda': params['gae_lambda'],\n",
    "        }\n",
    "\n",
    "        train_args = {\n",
    "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
    "        }\n",
    "\n",
    "        agent_params = {**computation_graph_args, **estimate_advantage_args, **train_args}\n",
    "\n",
    "        self.params = params\n",
    "        self.params['agent_class'] = PGAgent\n",
    "        self.params['agent_params'] = agent_params\n",
    "        self.params['batch_size_initial'] = self.params['batch_size']\n",
    "\n",
    "        ################\n",
    "        ## RL TRAINER\n",
    "        ################\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            self.params['n_iter'],\n",
    "            collect_policy = self.rl_trainer.agent.actor,\n",
    "            eval_policy = self.rl_trainer.agent.actor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2rCuQsRsd3N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\RL-homework\\hw2\\data\\q3_b40000_r0.005_LunarLanderContinuous-v2_09-09-2022_10-15-11\n",
      "########################\n",
      "logging outputs to  D:\\Code\\RL-homework\\hw2\\data\\q3_b40000_r0.005_LunarLanderContinuous-v2_09-09-2022_10-15-11\n",
      "########################\n",
      "Using GPU id 0\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -256.7511291503906\n",
      "Eval_StdReturn : 126.41618347167969\n",
      "Eval_MaxReturn : -71.04524993896484\n",
      "Eval_MinReturn : -397.1037902832031\n",
      "Eval_AverageEpLen : 117.5\n",
      "Train_AverageReturn : -325.2880554199219\n",
      "Train_StdReturn : 158.91587829589844\n",
      "Train_MaxReturn : -7.089210510253906\n",
      "Train_MinReturn : -774.9459228515625\n",
      "Train_AverageEpLen : 107.64247311827957\n",
      "Train_EnvstepsSoFar : 40043\n",
      "TimeSinceStart : 66.8566222190857\n",
      "Training Loss : -0.0015695743495598435\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -175.84060668945312\n",
      "Eval_StdReturn : 110.87763214111328\n",
      "Eval_MaxReturn : -94.2607650756836\n",
      "Eval_MinReturn : -391.714111328125\n",
      "Eval_AverageEpLen : 91.0\n",
      "Train_AverageReturn : -197.51751708984375\n",
      "Train_StdReturn : 120.91256713867188\n",
      "Train_MaxReturn : 43.21357727050781\n",
      "Train_MinReturn : -519.8556518554688\n",
      "Train_AverageEpLen : 98.55172413793103\n",
      "Train_EnvstepsSoFar : 80055\n",
      "TimeSinceStart : 134.121728181839\n",
      "Training Loss : -0.008716284297406673\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -74.1562728881836\n",
      "Eval_StdReturn : 49.8000602722168\n",
      "Eval_MaxReturn : 21.688644409179688\n",
      "Eval_MinReturn : -133.41937255859375\n",
      "Eval_AverageEpLen : 79.0\n",
      "Train_AverageReturn : -152.8201141357422\n",
      "Train_StdReturn : 91.2496109008789\n",
      "Train_MaxReturn : 107.65728759765625\n",
      "Train_MinReturn : -496.8984069824219\n",
      "Train_AverageEpLen : 92.09885057471264\n",
      "Train_EnvstepsSoFar : 120118\n",
      "TimeSinceStart : 199.73430585861206\n",
      "Training Loss : -0.006371255032718182\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -148.5323944091797\n",
      "Eval_StdReturn : 68.70045471191406\n",
      "Eval_MaxReturn : -98.55602264404297\n",
      "Eval_MinReturn : -266.77886962890625\n",
      "Eval_AverageEpLen : 106.25\n",
      "Train_AverageReturn : -133.63633728027344\n",
      "Train_StdReturn : 70.03559875488281\n",
      "Train_MaxReturn : 36.07011413574219\n",
      "Train_MinReturn : -423.6196594238281\n",
      "Train_AverageEpLen : 87.86622807017544\n",
      "Train_EnvstepsSoFar : 160185\n",
      "TimeSinceStart : 264.42103266716003\n",
      "Training Loss : 0.006101224571466446\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -102.74159240722656\n",
      "Eval_StdReturn : 35.85048294067383\n",
      "Eval_MaxReturn : -68.28495025634766\n",
      "Eval_MinReturn : -171.54815673828125\n",
      "Eval_AverageEpLen : 93.0\n",
      "Train_AverageReturn : -131.76966857910156\n",
      "Train_StdReturn : 63.31707763671875\n",
      "Train_MaxReturn : 28.477706909179688\n",
      "Train_MinReturn : -376.54498291015625\n",
      "Train_AverageEpLen : 86.57451403887688\n",
      "Train_EnvstepsSoFar : 200269\n",
      "TimeSinceStart : 328.88212609291077\n",
      "Training Loss : 0.004204283002763987\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -140.82150268554688\n",
      "Eval_StdReturn : 90.2186279296875\n",
      "Eval_MaxReturn : -64.39411926269531\n",
      "Eval_MinReturn : -330.8902587890625\n",
      "Eval_AverageEpLen : 76.5\n",
      "Train_AverageReturn : -125.23522186279297\n",
      "Train_StdReturn : 62.156044006347656\n",
      "Train_MaxReturn : 56.609771728515625\n",
      "Train_MinReturn : -373.3802490234375\n",
      "Train_AverageEpLen : 88.14096916299559\n",
      "Train_EnvstepsSoFar : 240285\n",
      "TimeSinceStart : 395.07349467277527\n",
      "Training Loss : 0.009777787141501904\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -183.9274139404297\n",
      "Eval_StdReturn : 135.02182006835938\n",
      "Eval_MaxReturn : -89.34591674804688\n",
      "Eval_MinReturn : -416.8448486328125\n",
      "Eval_AverageEpLen : 106.5\n",
      "Train_AverageReturn : -115.17606353759766\n",
      "Train_StdReturn : 52.334503173828125\n",
      "Train_MaxReturn : 34.58534240722656\n",
      "Train_MinReturn : -363.14935302734375\n",
      "Train_AverageEpLen : 88.19603524229075\n",
      "Train_EnvstepsSoFar : 280326\n",
      "TimeSinceStart : 465.0358097553253\n",
      "Training Loss : 0.004699319135397673\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -87.23558044433594\n",
      "Eval_StdReturn : 27.68885040283203\n",
      "Eval_MaxReturn : -45.24172592163086\n",
      "Eval_MinReturn : -112.6660385131836\n",
      "Eval_AverageEpLen : 89.6\n",
      "Train_AverageReturn : -103.46503448486328\n",
      "Train_StdReturn : 45.28382110595703\n",
      "Train_MaxReturn : 134.19366455078125\n",
      "Train_MinReturn : -356.842529296875\n",
      "Train_AverageEpLen : 89.40848214285714\n",
      "Train_EnvstepsSoFar : 320381\n",
      "TimeSinceStart : 533.4935581684113\n",
      "Training Loss : 0.004934989847242832\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -54.508018493652344\n",
      "Eval_StdReturn : 10.677742004394531\n",
      "Eval_MaxReturn : -44.569236755371094\n",
      "Eval_MinReturn : -75.07742309570312\n",
      "Eval_AverageEpLen : 93.8\n",
      "Train_AverageReturn : -97.1986312866211\n",
      "Train_StdReturn : 45.56965637207031\n",
      "Train_MaxReturn : 81.96388244628906\n",
      "Train_MinReturn : -336.580810546875\n",
      "Train_AverageEpLen : 94.69976359338061\n",
      "Train_EnvstepsSoFar : 360439\n",
      "TimeSinceStart : 601.6614940166473\n",
      "Training Loss : 0.010785207152366638\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -38.606475830078125\n",
      "Eval_StdReturn : 7.806919574737549\n",
      "Eval_MaxReturn : -28.446075439453125\n",
      "Eval_MinReturn : -50.16191101074219\n",
      "Eval_AverageEpLen : 123.75\n",
      "Train_AverageReturn : -74.61114501953125\n",
      "Train_StdReturn : 43.735904693603516\n",
      "Train_MaxReturn : 131.04718017578125\n",
      "Train_MinReturn : -303.3985595703125\n",
      "Train_AverageEpLen : 102.67179487179487\n",
      "Train_EnvstepsSoFar : 400481\n",
      "TimeSinceStart : 667.5528740882874\n",
      "Training Loss : 0.00023638966376893222\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -23.314319610595703\n",
      "Eval_StdReturn : 45.59238052368164\n",
      "Eval_MaxReturn : 22.836181640625\n",
      "Eval_MinReturn : -97.6065444946289\n",
      "Eval_AverageEpLen : 127.0\n",
      "Train_AverageReturn : -63.221336364746094\n",
      "Train_StdReturn : 42.064857482910156\n",
      "Train_MaxReturn : 144.4835205078125\n",
      "Train_MinReturn : -246.73712158203125\n",
      "Train_AverageEpLen : 109.6\n",
      "Train_EnvstepsSoFar : 440485\n",
      "TimeSinceStart : 733.3495724201202\n",
      "Training Loss : -0.002643860876560211\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -32.13405227661133\n",
      "Eval_StdReturn : 21.693525314331055\n",
      "Eval_MaxReturn : -11.554475784301758\n",
      "Eval_MinReturn : -69.2122573852539\n",
      "Eval_AverageEpLen : 103.2\n",
      "Train_AverageReturn : -52.14670944213867\n",
      "Train_StdReturn : 39.63774871826172\n",
      "Train_MaxReturn : 105.42784118652344\n",
      "Train_MinReturn : -230.30984497070312\n",
      "Train_AverageEpLen : 121.39393939393939\n",
      "Train_EnvstepsSoFar : 480545\n",
      "TimeSinceStart : 800.3530404567719\n",
      "Training Loss : -0.0005305457161739469\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -57.19842529296875\n",
      "Eval_StdReturn : 23.138315200805664\n",
      "Eval_MaxReturn : -29.25970458984375\n",
      "Eval_MinReturn : -85.92052459716797\n",
      "Eval_AverageEpLen : 147.66666666666666\n",
      "Train_AverageReturn : -41.359657287597656\n",
      "Train_StdReturn : 53.3221549987793\n",
      "Train_MaxReturn : 139.2895050048828\n",
      "Train_MinReturn : -274.65484619140625\n",
      "Train_AverageEpLen : 134.88888888888889\n",
      "Train_EnvstepsSoFar : 520607\n",
      "TimeSinceStart : 869.0053579807281\n",
      "Training Loss : -0.006363825406879187\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 63.8230094909668\n",
      "Eval_StdReturn : 42.54698181152344\n",
      "Eval_MaxReturn : 123.84577941894531\n",
      "Eval_MinReturn : 30.162155151367188\n",
      "Eval_AverageEpLen : 175.33333333333334\n",
      "Train_AverageReturn : -19.347124099731445\n",
      "Train_StdReturn : 66.61087799072266\n",
      "Train_MaxReturn : 195.76768493652344\n",
      "Train_MinReturn : -312.6674499511719\n",
      "Train_AverageEpLen : 162.76829268292684\n",
      "Train_EnvstepsSoFar : 560648\n",
      "TimeSinceStart : 942.6008670330048\n",
      "Training Loss : -0.004615101497620344\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -24.589994430541992\n",
      "Eval_StdReturn : 85.52507781982422\n",
      "Eval_MaxReturn : 65.32183074951172\n",
      "Eval_MinReturn : -139.60806274414062\n",
      "Eval_AverageEpLen : 191.33333333333334\n",
      "Train_AverageReturn : 4.184247970581055\n",
      "Train_StdReturn : 70.7993392944336\n",
      "Train_MaxReturn : 165.72158813476562\n",
      "Train_MinReturn : -196.09469604492188\n",
      "Train_AverageEpLen : 186.44651162790697\n",
      "Train_EnvstepsSoFar : 600734\n",
      "TimeSinceStart : 1017.9648332595825\n",
      "Training Loss : -0.0005584580940194428\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 10.856249809265137\n",
      "Eval_StdReturn : 29.08679962158203\n",
      "Eval_MaxReturn : 39.943050384521484\n",
      "Eval_MinReturn : -18.23055076599121\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 7.248903751373291\n",
      "Train_StdReturn : 64.31676483154297\n",
      "Train_MaxReturn : 137.70849609375\n",
      "Train_MinReturn : -197.21734619140625\n",
      "Train_AverageEpLen : 195.126213592233\n",
      "Train_EnvstepsSoFar : 640930\n",
      "TimeSinceStart : 1091.9625124931335\n",
      "Training Loss : -0.01463403645902872\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 49.321468353271484\n",
      "Eval_StdReturn : 21.8644962310791\n",
      "Eval_MaxReturn : 71.18596649169922\n",
      "Eval_MinReturn : 27.456972122192383\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 2.283853054046631\n",
      "Train_StdReturn : 51.0995979309082\n",
      "Train_MaxReturn : 114.85767364501953\n",
      "Train_MinReturn : -162.63265991210938\n",
      "Train_AverageEpLen : 198.28217821782178\n",
      "Train_EnvstepsSoFar : 680983\n",
      "TimeSinceStart : 1167.0087144374847\n",
      "Training Loss : -0.005002764519304037\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -18.11482048034668\n",
      "Eval_StdReturn : 12.312582015991211\n",
      "Eval_MaxReturn : -5.802238464355469\n",
      "Eval_MinReturn : -30.42740249633789\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 16.497777938842773\n",
      "Train_StdReturn : 49.59238052368164\n",
      "Train_MaxReturn : 135.08709716796875\n",
      "Train_MinReturn : -168.5354766845703\n",
      "Train_AverageEpLen : 198.8019801980198\n",
      "Train_EnvstepsSoFar : 721141\n",
      "TimeSinceStart : 1243.4417531490326\n",
      "Training Loss : -0.0024353936314582825\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 30.225805282592773\n",
      "Eval_StdReturn : 12.527780532836914\n",
      "Eval_MaxReturn : 42.75358581542969\n",
      "Eval_MinReturn : 17.69802474975586\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 34.13776779174805\n",
      "Train_StdReturn : 39.19995880126953\n",
      "Train_MaxReturn : 128.99473571777344\n",
      "Train_MinReturn : -168.0131072998047\n",
      "Train_AverageEpLen : 199.7860696517413\n",
      "Train_EnvstepsSoFar : 761298\n",
      "TimeSinceStart : 1318.4284114837646\n",
      "Training Loss : -0.0070159886963665485\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 80.66130065917969\n",
      "Eval_StdReturn : 12.528732299804688\n",
      "Eval_MaxReturn : 93.19003295898438\n",
      "Eval_MinReturn : 68.132568359375\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 56.61653137207031\n",
      "Train_StdReturn : 30.724567413330078\n",
      "Train_MaxReturn : 152.73724365234375\n",
      "Train_MinReturn : -43.400848388671875\n",
      "Train_AverageEpLen : 199.6318407960199\n",
      "Train_EnvstepsSoFar : 801424\n",
      "TimeSinceStart : 1393.3780772686005\n",
      "Training Loss : -0.002015929203480482\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 76.95060729980469\n",
      "Eval_StdReturn : 5.51483154296875\n",
      "Eval_MaxReturn : 82.46543884277344\n",
      "Eval_MinReturn : 71.43577575683594\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 72.69129180908203\n",
      "Train_StdReturn : 31.593969345092773\n",
      "Train_MaxReturn : 136.76760864257812\n",
      "Train_MinReturn : -52.48451232910156\n",
      "Train_AverageEpLen : 199.34825870646767\n",
      "Train_EnvstepsSoFar : 841493\n",
      "TimeSinceStart : 1467.7850320339203\n",
      "Training Loss : -0.00188155728392303\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 84.90436553955078\n",
      "Eval_StdReturn : 46.06324005126953\n",
      "Eval_MaxReturn : 138.63565063476562\n",
      "Eval_MinReturn : 26.141315460205078\n",
      "Eval_AverageEpLen : 197.0\n",
      "Train_AverageReturn : 86.428466796875\n",
      "Train_StdReturn : 27.57396125793457\n",
      "Train_MaxReturn : 161.26947021484375\n",
      "Train_MinReturn : -17.78036117553711\n",
      "Train_AverageEpLen : 198.71782178217822\n",
      "Train_EnvstepsSoFar : 881634\n",
      "TimeSinceStart : 1544.6555335521698\n",
      "Training Loss : -0.010680499486625195\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 137.30908203125\n",
      "Eval_StdReturn : 4.8438720703125\n",
      "Eval_MaxReturn : 142.1529541015625\n",
      "Eval_MinReturn : 132.4652099609375\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 89.22388458251953\n",
      "Train_StdReturn : 34.26490020751953\n",
      "Train_MaxReturn : 155.27835083007812\n",
      "Train_MinReturn : -26.87708854675293\n",
      "Train_AverageEpLen : 196.81372549019608\n",
      "Train_EnvstepsSoFar : 921784\n",
      "TimeSinceStart : 1619.7206478118896\n",
      "Training Loss : -0.006290041375905275\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 102.77079772949219\n",
      "Eval_StdReturn : 1.4094810485839844\n",
      "Eval_MaxReturn : 104.18028259277344\n",
      "Eval_MinReturn : 101.36132049560547\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 99.27141571044922\n",
      "Train_StdReturn : 40.47768783569336\n",
      "Train_MaxReturn : 178.0193634033203\n",
      "Train_MinReturn : -101.46434020996094\n",
      "Train_AverageEpLen : 193.2753623188406\n",
      "Train_EnvstepsSoFar : 961792\n",
      "TimeSinceStart : 1694.8381032943726\n",
      "Training Loss : 0.001736113685183227\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 111.27203369140625\n",
      "Eval_StdReturn : 6.298099517822266\n",
      "Eval_MaxReturn : 117.57012939453125\n",
      "Eval_MinReturn : 104.97393035888672\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 91.53543090820312\n",
      "Train_StdReturn : 41.5352668762207\n",
      "Train_MaxReturn : 168.30599975585938\n",
      "Train_MinReturn : -17.122238159179688\n",
      "Train_AverageEpLen : 187.07476635514018\n",
      "Train_EnvstepsSoFar : 1001826\n",
      "TimeSinceStart : 1768.8257701396942\n",
      "Training Loss : -0.0048322053626179695\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 113.93354797363281\n",
      "Eval_StdReturn : 8.311782836914062\n",
      "Eval_MaxReturn : 122.24533081054688\n",
      "Eval_MinReturn : 105.62176513671875\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 98.47687530517578\n",
      "Train_StdReturn : 40.349002838134766\n",
      "Train_MaxReturn : 170.0685577392578\n",
      "Train_MinReturn : -16.56357765197754\n",
      "Train_AverageEpLen : 188.18309859154928\n",
      "Train_EnvstepsSoFar : 1041909\n",
      "TimeSinceStart : 1843.0569860935211\n",
      "Training Loss : -0.0016532604349777102\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 103.11497497558594\n",
      "Eval_StdReturn : 10.544483184814453\n",
      "Eval_MaxReturn : 113.65945434570312\n",
      "Eval_MinReturn : 92.57048797607422\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 111.22791290283203\n",
      "Train_StdReturn : 31.319948196411133\n",
      "Train_MaxReturn : 159.7837371826172\n",
      "Train_MinReturn : -2.8576297760009766\n",
      "Train_AverageEpLen : 195.37560975609756\n",
      "Train_EnvstepsSoFar : 1081961\n",
      "TimeSinceStart : 1917.9751405715942\n",
      "Training Loss : 0.002046641893684864\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 108.33494567871094\n",
      "Eval_StdReturn : 15.08896255493164\n",
      "Eval_MaxReturn : 123.42391204833984\n",
      "Eval_MinReturn : 93.24598693847656\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 110.1982650756836\n",
      "Train_StdReturn : 28.006404876708984\n",
      "Train_MaxReturn : 176.55621337890625\n",
      "Train_MinReturn : 9.240280151367188\n",
      "Train_AverageEpLen : 196.1813725490196\n",
      "Train_EnvstepsSoFar : 1121982\n",
      "TimeSinceStart : 1990.5085263252258\n",
      "Training Loss : -0.005096700508147478\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 103.17584228515625\n",
      "Eval_StdReturn : 13.269168853759766\n",
      "Eval_MaxReturn : 116.44501495361328\n",
      "Eval_MinReturn : 89.90667724609375\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 115.00962829589844\n",
      "Train_StdReturn : 27.876237869262695\n",
      "Train_MaxReturn : 166.76473999023438\n",
      "Train_MinReturn : 6.872589111328125\n",
      "Train_AverageEpLen : 198.0\n",
      "Train_EnvstepsSoFar : 1162176\n",
      "TimeSinceStart : 2061.5008006095886\n",
      "Training Loss : 0.0028983871452510357\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 131.94410705566406\n",
      "Eval_StdReturn : 12.135364532470703\n",
      "Eval_MaxReturn : 144.0794677734375\n",
      "Eval_MinReturn : 119.8087387084961\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 116.09326934814453\n",
      "Train_StdReturn : 26.023738861083984\n",
      "Train_MaxReturn : 169.0958251953125\n",
      "Train_MinReturn : 0.27007293701171875\n",
      "Train_AverageEpLen : 198.15346534653466\n",
      "Train_EnvstepsSoFar : 1202203\n",
      "TimeSinceStart : 2132.5582234859467\n",
      "Training Loss : -0.015470239333808422\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 120.45257568359375\n",
      "Eval_StdReturn : 8.505245208740234\n",
      "Eval_MaxReturn : 128.95782470703125\n",
      "Eval_MinReturn : 111.94733428955078\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 119.97067260742188\n",
      "Train_StdReturn : 20.633684158325195\n",
      "Train_MaxReturn : 171.78353881835938\n",
      "Train_MinReturn : 27.57864761352539\n",
      "Train_AverageEpLen : 199.9452736318408\n",
      "Train_EnvstepsSoFar : 1242392\n",
      "TimeSinceStart : 2203.2674157619476\n",
      "Training Loss : 0.0007721663569100201\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 126.40343475341797\n",
      "Eval_StdReturn : 3.5107955932617188\n",
      "Eval_MaxReturn : 129.9142303466797\n",
      "Eval_MinReturn : 122.89263916015625\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 122.94195556640625\n",
      "Train_StdReturn : 19.625688552856445\n",
      "Train_MaxReturn : 163.93556213378906\n",
      "Train_MinReturn : 73.7437744140625\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1282392\n",
      "TimeSinceStart : 2274.2659804821014\n",
      "Training Loss : -0.0037238341756165028\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 143.53469848632812\n",
      "Eval_StdReturn : 14.80450439453125\n",
      "Eval_MaxReturn : 158.33920288085938\n",
      "Eval_MinReturn : 128.73019409179688\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 117.6807632446289\n",
      "Train_StdReturn : 19.670989990234375\n",
      "Train_MaxReturn : 174.11351013183594\n",
      "Train_MinReturn : 65.156005859375\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1322392\n",
      "TimeSinceStart : 2344.9717559814453\n",
      "Training Loss : -0.004241201560944319\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 135.4307403564453\n",
      "Eval_StdReturn : 13.650192260742188\n",
      "Eval_MaxReturn : 149.0809326171875\n",
      "Eval_MinReturn : 121.78054809570312\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 122.42037200927734\n",
      "Train_StdReturn : 19.824373245239258\n",
      "Train_MaxReturn : 168.03602600097656\n",
      "Train_MinReturn : 70.96908569335938\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1362392\n",
      "TimeSinceStart : 2415.5117173194885\n",
      "Training Loss : -0.01119990088045597\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 112.57280731201172\n",
      "Eval_StdReturn : 21.94373321533203\n",
      "Eval_MaxReturn : 134.51654052734375\n",
      "Eval_MinReturn : 90.62907409667969\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 122.67465209960938\n",
      "Train_StdReturn : 21.282686233520508\n",
      "Train_MaxReturn : 180.17019653320312\n",
      "Train_MinReturn : 74.81755065917969\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1402392\n",
      "TimeSinceStart : 2485.891222000122\n",
      "Training Loss : 0.0020645195618271828\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 134.43882751464844\n",
      "Eval_StdReturn : 16.26495361328125\n",
      "Eval_MaxReturn : 150.7037811279297\n",
      "Eval_MinReturn : 118.17387390136719\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 119.82927703857422\n",
      "Train_StdReturn : 20.6404972076416\n",
      "Train_MaxReturn : 169.21588134765625\n",
      "Train_MinReturn : 75.24114990234375\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1442392\n",
      "TimeSinceStart : 2555.95406126976\n",
      "Training Loss : -0.010273057036101818\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 135.09039306640625\n",
      "Eval_StdReturn : 0.5851364135742188\n",
      "Eval_MaxReturn : 135.675537109375\n",
      "Eval_MinReturn : 134.50526428222656\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 123.38762664794922\n",
      "Train_StdReturn : 20.780357360839844\n",
      "Train_MaxReturn : 175.37303161621094\n",
      "Train_MinReturn : 55.8602409362793\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1482392\n",
      "TimeSinceStart : 2626.643702030182\n",
      "Training Loss : -0.00539680290967226\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 125.26750183105469\n",
      "Eval_StdReturn : 21.421951293945312\n",
      "Eval_MaxReturn : 146.689453125\n",
      "Eval_MinReturn : 103.84555053710938\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 123.65205383300781\n",
      "Train_StdReturn : 19.934968948364258\n",
      "Train_MaxReturn : 172.14260864257812\n",
      "Train_MinReturn : 74.94032287597656\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1522392\n",
      "TimeSinceStart : 2697.1719579696655\n",
      "Training Loss : -0.0008277389570139349\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 116.660888671875\n",
      "Eval_StdReturn : 15.892105102539062\n",
      "Eval_MaxReturn : 132.55299377441406\n",
      "Eval_MinReturn : 100.76878356933594\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 123.79583740234375\n",
      "Train_StdReturn : 20.653852462768555\n",
      "Train_MaxReturn : 180.87689208984375\n",
      "Train_MinReturn : 74.63837432861328\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1562392\n",
      "TimeSinceStart : 2767.603899240494\n",
      "Training Loss : -0.0026164413429796696\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 131.09136962890625\n",
      "Eval_StdReturn : 28.132232666015625\n",
      "Eval_MaxReturn : 159.22360229492188\n",
      "Eval_MinReturn : 102.95913696289062\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 129.30380249023438\n",
      "Train_StdReturn : 22.499103546142578\n",
      "Train_MaxReturn : 183.8948974609375\n",
      "Train_MinReturn : 73.15558624267578\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1602392\n",
      "TimeSinceStart : 2839.0568385124207\n",
      "Training Loss : -0.01078539714217186\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 156.75592041015625\n",
      "Eval_StdReturn : 1.3821334838867188\n",
      "Eval_MaxReturn : 158.1380615234375\n",
      "Eval_MinReturn : 155.37379455566406\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 127.64181518554688\n",
      "Train_StdReturn : 20.12306022644043\n",
      "Train_MaxReturn : 176.8744659423828\n",
      "Train_MinReturn : 86.8064956665039\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1642392\n",
      "TimeSinceStart : 2910.179163455963\n",
      "Training Loss : 0.010064100846648216\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 155.1581268310547\n",
      "Eval_StdReturn : 0.9883575439453125\n",
      "Eval_MaxReturn : 156.146484375\n",
      "Eval_MinReturn : 154.16976928710938\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 133.13563537597656\n",
      "Train_StdReturn : 21.591739654541016\n",
      "Train_MaxReturn : 184.6348419189453\n",
      "Train_MinReturn : 82.0798568725586\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 1682392\n",
      "TimeSinceStart : 2980.6222558021545\n",
      "Training Loss : 0.00044473379966802895\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 134.14942932128906\n",
      "Eval_StdReturn : 14.799728393554688\n",
      "Eval_MaxReturn : 148.94915771484375\n",
      "Eval_MinReturn : 119.34970092773438\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 135.81472778320312\n",
      "Train_StdReturn : 20.36794662475586\n",
      "Train_MaxReturn : 193.59494018554688\n",
      "Train_MinReturn : 77.20436096191406\n",
      "Train_AverageEpLen : 199.681592039801\n",
      "Train_EnvstepsSoFar : 1722528\n",
      "TimeSinceStart : 3051.3109250068665\n",
      "Training Loss : -0.0018237308831885457\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 117.39157104492188\n",
      "Eval_StdReturn : 1.1991424560546875\n",
      "Eval_MaxReturn : 118.59071350097656\n",
      "Eval_MinReturn : 116.19242858886719\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 138.8650360107422\n",
      "Train_StdReturn : 20.832075119018555\n",
      "Train_MaxReturn : 185.21621704101562\n",
      "Train_MinReturn : 50.05293273925781\n",
      "Train_AverageEpLen : 199.73134328358208\n",
      "Train_EnvstepsSoFar : 1762674\n",
      "TimeSinceStart : 3122.4653203487396\n",
      "Training Loss : 0.003999587148427963\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 159.1786651611328\n",
      "Eval_StdReturn : 18.286575317382812\n",
      "Eval_MaxReturn : 177.46524047851562\n",
      "Eval_MinReturn : 140.89208984375\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 138.12303161621094\n",
      "Train_StdReturn : 22.764657974243164\n",
      "Train_MaxReturn : 189.4617919921875\n",
      "Train_MinReturn : 44.18693542480469\n",
      "Train_AverageEpLen : 198.43069306930693\n",
      "Train_EnvstepsSoFar : 1802757\n",
      "TimeSinceStart : 3193.277631998062\n",
      "Training Loss : -0.009270892478525639\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 140.20843505859375\n",
      "Eval_StdReturn : 16.14234161376953\n",
      "Eval_MaxReturn : 156.35076904296875\n",
      "Eval_MinReturn : 124.06608581542969\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 136.2943572998047\n",
      "Train_StdReturn : 29.867773056030273\n",
      "Train_MaxReturn : 197.35752868652344\n",
      "Train_MinReturn : -14.633752822875977\n",
      "Train_AverageEpLen : 197.95073891625617\n",
      "Train_EnvstepsSoFar : 1842941\n",
      "TimeSinceStart : 3263.9620110988617\n",
      "Training Loss : 0.005329931154847145\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 127.13899230957031\n",
      "Eval_StdReturn : 18.08767318725586\n",
      "Eval_MaxReturn : 145.22666931152344\n",
      "Eval_MinReturn : 109.05132293701172\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 136.9447784423828\n",
      "Train_StdReturn : 32.200531005859375\n",
      "Train_MaxReturn : 203.27928161621094\n",
      "Train_MinReturn : 19.13671112060547\n",
      "Train_AverageEpLen : 195.81463414634146\n",
      "Train_EnvstepsSoFar : 1883083\n",
      "TimeSinceStart : 3337.8006896972656\n",
      "Training Loss : -0.0022816979326307774\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 147.28033447265625\n",
      "Eval_StdReturn : 44.49400329589844\n",
      "Eval_MaxReturn : 191.7743377685547\n",
      "Eval_MinReturn : 102.78633117675781\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 133.817138671875\n",
      "Train_StdReturn : 31.983753204345703\n",
      "Train_MaxReturn : 185.49139404296875\n",
      "Train_MinReturn : 4.663665771484375\n",
      "Train_AverageEpLen : 193.71014492753622\n",
      "Train_EnvstepsSoFar : 1923181\n",
      "TimeSinceStart : 3412.13369512558\n",
      "Training Loss : -0.0005755346501246095\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 151.38088989257812\n",
      "Eval_StdReturn : 2.913909912109375\n",
      "Eval_MaxReturn : 154.2947998046875\n",
      "Eval_MinReturn : 148.46697998046875\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 134.0237579345703\n",
      "Train_StdReturn : 32.188411712646484\n",
      "Train_MaxReturn : 192.081787109375\n",
      "Train_MinReturn : 0.6373062133789062\n",
      "Train_AverageEpLen : 194.873786407767\n",
      "Train_EnvstepsSoFar : 1963325\n",
      "TimeSinceStart : 3487.298284292221\n",
      "Training Loss : 0.005223641637712717\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 150.0431671142578\n",
      "Eval_StdReturn : 13.635971069335938\n",
      "Eval_MaxReturn : 163.67913818359375\n",
      "Eval_MinReturn : 136.40719604492188\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 137.8183135986328\n",
      "Train_StdReturn : 27.574329376220703\n",
      "Train_MaxReturn : 192.9508056640625\n",
      "Train_MinReturn : 29.9689884185791\n",
      "Train_AverageEpLen : 197.0985221674877\n",
      "Train_EnvstepsSoFar : 2003336\n",
      "TimeSinceStart : 3562.4501516819\n",
      "Training Loss : -0.004572248086333275\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 164.18795776367188\n",
      "Eval_StdReturn : 14.633460998535156\n",
      "Eval_MaxReturn : 178.8214111328125\n",
      "Eval_MinReturn : 149.5544891357422\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 139.3897247314453\n",
      "Train_StdReturn : 29.849891662597656\n",
      "Train_MaxReturn : 190.77027893066406\n",
      "Train_MinReturn : 13.240516662597656\n",
      "Train_AverageEpLen : 196.27941176470588\n",
      "Train_EnvstepsSoFar : 2043377\n",
      "TimeSinceStart : 3634.095717191696\n",
      "Training Loss : -0.004947760608047247\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 163.4643096923828\n",
      "Eval_StdReturn : 28.716384887695312\n",
      "Eval_MaxReturn : 192.18069458007812\n",
      "Eval_MinReturn : 134.7479248046875\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 140.91909790039062\n",
      "Train_StdReturn : 25.397632598876953\n",
      "Train_MaxReturn : 189.91050720214844\n",
      "Train_MinReturn : 24.830368041992188\n",
      "Train_AverageEpLen : 198.15841584158414\n",
      "Train_EnvstepsSoFar : 2083405\n",
      "TimeSinceStart : 3705.56681227684\n",
      "Training Loss : -0.007483093999326229\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 177.71197509765625\n",
      "Eval_StdReturn : 7.266746520996094\n",
      "Eval_MaxReturn : 184.97872924804688\n",
      "Eval_MinReturn : 170.4452362060547\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 145.29888916015625\n",
      "Train_StdReturn : 20.814510345458984\n",
      "Train_MaxReturn : 203.85409545898438\n",
      "Train_MinReturn : 42.413265228271484\n",
      "Train_AverageEpLen : 199.70149253731344\n",
      "Train_EnvstepsSoFar : 2123545\n",
      "TimeSinceStart : 3779.941219806671\n",
      "Training Loss : -0.004854870494455099\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 126.37620544433594\n",
      "Eval_StdReturn : 4.765064239501953\n",
      "Eval_MaxReturn : 131.14126586914062\n",
      "Eval_MinReturn : 121.61113739013672\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 147.8292236328125\n",
      "Train_StdReturn : 21.975679397583008\n",
      "Train_MaxReturn : 201.75582885742188\n",
      "Train_MinReturn : 76.82443237304688\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 2163545\n",
      "TimeSinceStart : 3850.650961637497\n",
      "Training Loss : 0.007525517139583826\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 162.96368408203125\n",
      "Eval_StdReturn : 1.578582763671875\n",
      "Eval_MaxReturn : 164.54226684570312\n",
      "Eval_MinReturn : 161.38510131835938\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 150.4763641357422\n",
      "Train_StdReturn : 18.672565460205078\n",
      "Train_MaxReturn : 194.63442993164062\n",
      "Train_MinReturn : 93.31854248046875\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 2203545\n",
      "TimeSinceStart : 3921.5393466949463\n",
      "Training Loss : -0.003265107749029994\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 141.856689453125\n",
      "Eval_StdReturn : 4.602142333984375\n",
      "Eval_MaxReturn : 146.45883178710938\n",
      "Eval_MinReturn : 137.25454711914062\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 144.78468322753906\n",
      "Train_StdReturn : 23.14937400817871\n",
      "Train_MaxReturn : 199.5718231201172\n",
      "Train_MinReturn : 45.05865478515625\n",
      "Train_AverageEpLen : 199.09452736318408\n",
      "Train_EnvstepsSoFar : 2243563\n",
      "TimeSinceStart : 3992.1057291030884\n",
      "Training Loss : -0.0008666433859616518\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 138.58261108398438\n",
      "Eval_StdReturn : 1.0507431030273438\n",
      "Eval_MaxReturn : 139.63336181640625\n",
      "Eval_MinReturn : 137.53187561035156\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 147.71258544921875\n",
      "Train_StdReturn : 22.17413902282715\n",
      "Train_MaxReturn : 195.9482879638672\n",
      "Train_MinReturn : 53.628082275390625\n",
      "Train_AverageEpLen : 199.50746268656715\n",
      "Train_EnvstepsSoFar : 2283664\n",
      "TimeSinceStart : 4062.4829292297363\n",
      "Training Loss : -0.008175188675522804\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 140.76153564453125\n",
      "Eval_StdReturn : 18.302391052246094\n",
      "Eval_MaxReturn : 159.0639190673828\n",
      "Eval_MinReturn : 122.45913696289062\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 147.6298370361328\n",
      "Train_StdReturn : 20.718788146972656\n",
      "Train_MaxReturn : 194.8209686279297\n",
      "Train_MinReturn : 87.02000427246094\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 2323664\n",
      "TimeSinceStart : 4136.749664068222\n",
      "Training Loss : -0.000542136374861002\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 143.99557495117188\n",
      "Eval_StdReturn : 18.460105895996094\n",
      "Eval_MaxReturn : 162.45567321777344\n",
      "Eval_MinReturn : 125.53546142578125\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 148.4576416015625\n",
      "Train_StdReturn : 21.992347717285156\n",
      "Train_MaxReturn : 195.04824829101562\n",
      "Train_MinReturn : 93.91168212890625\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 2363664\n",
      "TimeSinceStart : 4206.3709127902985\n",
      "Training Loss : -0.00209804461337626\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 138.0752410888672\n",
      "Eval_StdReturn : 1.80841064453125\n",
      "Eval_MaxReturn : 139.88365173339844\n",
      "Eval_MinReturn : 136.26683044433594\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 150.1292724609375\n",
      "Train_StdReturn : 21.24966812133789\n",
      "Train_MaxReturn : 203.62484741210938\n",
      "Train_MinReturn : 88.29190063476562\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 2403664\n",
      "TimeSinceStart : 4277.97995018959\n",
      "Training Loss : -0.003948938101530075\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 163.704345703125\n",
      "Eval_StdReturn : 3.9169464111328125\n",
      "Eval_MaxReturn : 167.6212921142578\n",
      "Eval_MinReturn : 159.7873992919922\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 149.1290283203125\n",
      "Train_StdReturn : 21.851716995239258\n",
      "Train_MaxReturn : 196.96043395996094\n",
      "Train_MinReturn : 103.40504455566406\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 2443664\n",
      "TimeSinceStart : 4348.563066959381\n",
      "Training Loss : 0.0007892688736319542\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 165.22613525390625\n",
      "Eval_StdReturn : 21.34564208984375\n",
      "Eval_MaxReturn : 186.57177734375\n",
      "Eval_MinReturn : 143.8804931640625\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 151.0839385986328\n",
      "Train_StdReturn : 23.030731201171875\n",
      "Train_MaxReturn : 198.10855102539062\n",
      "Train_MinReturn : 28.699966430664062\n",
      "Train_AverageEpLen : 199.70149253731344\n",
      "Train_EnvstepsSoFar : 2483804\n",
      "TimeSinceStart : 4419.475466489792\n",
      "Training Loss : 0.0005856683128513396\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 166.7937774658203\n",
      "Eval_StdReturn : 0.7426605224609375\n",
      "Eval_MaxReturn : 167.53643798828125\n",
      "Eval_MinReturn : 166.05111694335938\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 149.62881469726562\n",
      "Train_StdReturn : 24.71025276184082\n",
      "Train_MaxReturn : 200.26141357421875\n",
      "Train_MinReturn : 49.125877380371094\n",
      "Train_AverageEpLen : 199.39800995024876\n",
      "Train_EnvstepsSoFar : 2523883\n",
      "TimeSinceStart : 4491.297464132309\n",
      "Training Loss : -0.003676694817841053\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 170.96630859375\n",
      "Eval_StdReturn : 6.0362548828125\n",
      "Eval_MaxReturn : 177.0025634765625\n",
      "Eval_MinReturn : 164.9300537109375\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 150.5745849609375\n",
      "Train_StdReturn : 22.603261947631836\n",
      "Train_MaxReturn : 211.7900390625\n",
      "Train_MinReturn : 36.510986328125\n",
      "Train_AverageEpLen : 199.4278606965174\n",
      "Train_EnvstepsSoFar : 2563968\n",
      "TimeSinceStart : 4562.626513719559\n",
      "Training Loss : -0.0001475430908612907\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 132.0651397705078\n",
      "Eval_StdReturn : 1.3201446533203125\n",
      "Eval_MaxReturn : 133.38528442382812\n",
      "Eval_MinReturn : 130.7449951171875\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 147.9850311279297\n",
      "Train_StdReturn : 24.353483200073242\n",
      "Train_MaxReturn : 199.53939819335938\n",
      "Train_MinReturn : 38.326385498046875\n",
      "Train_AverageEpLen : 199.6318407960199\n",
      "Train_EnvstepsSoFar : 2604094\n",
      "TimeSinceStart : 4635.041276931763\n",
      "Training Loss : -0.006839175708591938\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 162.52001953125\n",
      "Eval_StdReturn : 11.012893676757812\n",
      "Eval_MaxReturn : 173.5329132080078\n",
      "Eval_MinReturn : 151.5071258544922\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 144.17726135253906\n",
      "Train_StdReturn : 29.041173934936523\n",
      "Train_MaxReturn : 200.57325744628906\n",
      "Train_MinReturn : 51.74664306640625\n",
      "Train_AverageEpLen : 196.2549019607843\n",
      "Train_EnvstepsSoFar : 2644130\n",
      "TimeSinceStart : 4706.641587734222\n",
      "Training Loss : -0.008051935583353043\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 99.82242584228516\n",
      "Eval_StdReturn : 60.29482650756836\n",
      "Eval_MaxReturn : 185.06520080566406\n",
      "Eval_MinReturn : 55.3433837890625\n",
      "Eval_AverageEpLen : 152.66666666666666\n",
      "Train_AverageReturn : 146.54080200195312\n",
      "Train_StdReturn : 30.0245361328125\n",
      "Train_MaxReturn : 203.41380310058594\n",
      "Train_MinReturn : 31.975507736206055\n",
      "Train_AverageEpLen : 196.52450980392157\n",
      "Train_EnvstepsSoFar : 2684221\n",
      "TimeSinceStart : 4779.407843112946\n",
      "Training Loss : 0.0003629267157521099\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 173.95571899414062\n",
      "Eval_StdReturn : 8.939674377441406\n",
      "Eval_MaxReturn : 182.8953857421875\n",
      "Eval_MinReturn : 165.0160369873047\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 136.72463989257812\n",
      "Train_StdReturn : 43.42837142944336\n",
      "Train_MaxReturn : 198.8282012939453\n",
      "Train_MinReturn : 14.364089965820312\n",
      "Train_AverageEpLen : 188.31455399061034\n",
      "Train_EnvstepsSoFar : 2724332\n",
      "TimeSinceStart : 4850.194980144501\n",
      "Training Loss : -0.0022351995576173067\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 159.68740844726562\n",
      "Eval_StdReturn : 13.021919250488281\n",
      "Eval_MaxReturn : 172.70932006835938\n",
      "Eval_MinReturn : 146.6654815673828\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 132.92625427246094\n",
      "Train_StdReturn : 42.29235076904297\n",
      "Train_MaxReturn : 203.7236328125\n",
      "Train_MinReturn : 7.220966339111328\n",
      "Train_AverageEpLen : 187.20093457943926\n",
      "Train_EnvstepsSoFar : 2764393\n",
      "TimeSinceStart : 4921.46830701828\n",
      "Training Loss : -0.0022098517511039972\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 166.69944763183594\n",
      "Eval_StdReturn : 0.3724212646484375\n",
      "Eval_MaxReturn : 167.07186889648438\n",
      "Eval_MinReturn : 166.3270263671875\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 135.77047729492188\n",
      "Train_StdReturn : 42.34968948364258\n",
      "Train_MaxReturn : 203.2378692626953\n",
      "Train_MinReturn : 19.449012756347656\n",
      "Train_AverageEpLen : 188.61971830985917\n",
      "Train_EnvstepsSoFar : 2804569\n",
      "TimeSinceStart : 4991.93541097641\n",
      "Training Loss : -0.0008805676479823887\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 178.13648986816406\n",
      "Eval_StdReturn : 25.017990112304688\n",
      "Eval_MaxReturn : 203.15447998046875\n",
      "Eval_MinReturn : 153.11849975585938\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 143.70068359375\n",
      "Train_StdReturn : 34.979679107666016\n",
      "Train_MaxReturn : 208.69444274902344\n",
      "Train_MinReturn : 32.448326110839844\n",
      "Train_AverageEpLen : 194.03864734299518\n",
      "Train_EnvstepsSoFar : 2844735\n",
      "TimeSinceStart : 5062.893361568451\n",
      "Training Loss : 0.0056204055435955524\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 151.43006896972656\n",
      "Eval_StdReturn : 28.140304565429688\n",
      "Eval_MaxReturn : 179.57037353515625\n",
      "Eval_MinReturn : 123.28976440429688\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 149.7137451171875\n",
      "Train_StdReturn : 31.09330940246582\n",
      "Train_MaxReturn : 211.07510375976562\n",
      "Train_MinReturn : -1.3607673645019531\n",
      "Train_AverageEpLen : 197.17241379310346\n",
      "Train_EnvstepsSoFar : 2884761\n",
      "TimeSinceStart : 5134.154625177383\n",
      "Training Loss : 0.00180958048440516\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 151.30642700195312\n",
      "Eval_StdReturn : 2.0523147583007812\n",
      "Eval_MaxReturn : 153.35874938964844\n",
      "Eval_MinReturn : 149.25411987304688\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 157.7811737060547\n",
      "Train_StdReturn : 25.22322654724121\n",
      "Train_MaxReturn : 213.19383239746094\n",
      "Train_MinReturn : 46.04110336303711\n",
      "Train_AverageEpLen : 199.13432835820896\n",
      "Train_EnvstepsSoFar : 2924787\n",
      "TimeSinceStart : 5205.753385543823\n",
      "Training Loss : 0.0035594662185758352\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 159.1398162841797\n",
      "Eval_StdReturn : 9.505813598632812\n",
      "Eval_MaxReturn : 168.6456298828125\n",
      "Eval_MinReturn : 149.63400268554688\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 160.10055541992188\n",
      "Train_StdReturn : 25.57866096496582\n",
      "Train_MaxReturn : 215.02947998046875\n",
      "Train_MinReturn : 48.801788330078125\n",
      "Train_AverageEpLen : 199.08955223880596\n",
      "Train_EnvstepsSoFar : 2964804\n",
      "TimeSinceStart : 5277.255055427551\n",
      "Training Loss : -0.004754764027893543\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 188.2173614501953\n",
      "Eval_StdReturn : 12.623764038085938\n",
      "Eval_MaxReturn : 200.84112548828125\n",
      "Eval_MinReturn : 175.59359741210938\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 162.1353759765625\n",
      "Train_StdReturn : 27.725435256958008\n",
      "Train_MaxReturn : 222.1535186767578\n",
      "Train_MinReturn : 38.35948181152344\n",
      "Train_AverageEpLen : 199.1144278606965\n",
      "Train_EnvstepsSoFar : 3004826\n",
      "TimeSinceStart : 5348.064101457596\n",
      "Training Loss : 0.006306936964392662\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 187.96343994140625\n",
      "Eval_StdReturn : 7.145782470703125\n",
      "Eval_MaxReturn : 195.10922241210938\n",
      "Eval_MinReturn : 180.81765747070312\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 160.82107543945312\n",
      "Train_StdReturn : 27.99234390258789\n",
      "Train_MaxReturn : 209.36062622070312\n",
      "Train_MinReturn : 45.889610290527344\n",
      "Train_AverageEpLen : 199.00497512437812\n",
      "Train_EnvstepsSoFar : 3044826\n",
      "TimeSinceStart : 5419.575520038605\n",
      "Training Loss : -0.007159010041505098\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 160.60775756835938\n",
      "Eval_StdReturn : 4.253852844238281\n",
      "Eval_MaxReturn : 164.8616180419922\n",
      "Eval_MinReturn : 156.35391235351562\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 163.87220764160156\n",
      "Train_StdReturn : 24.207942962646484\n",
      "Train_MaxReturn : 221.971435546875\n",
      "Train_MinReturn : 41.8724250793457\n",
      "Train_AverageEpLen : 199.76616915422886\n",
      "Train_EnvstepsSoFar : 3084979\n",
      "TimeSinceStart : 5491.8926429748535\n",
      "Training Loss : 0.00044660866842605174\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 154.9618682861328\n",
      "Eval_StdReturn : 7.6015625\n",
      "Eval_MaxReturn : 162.5634307861328\n",
      "Eval_MinReturn : 147.3603057861328\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 163.9848175048828\n",
      "Train_StdReturn : 29.853132247924805\n",
      "Train_MaxReturn : 221.55935668945312\n",
      "Train_MinReturn : 38.21434020996094\n",
      "Train_AverageEpLen : 198.41584158415841\n",
      "Train_EnvstepsSoFar : 3125059\n",
      "TimeSinceStart : 5563.802943229675\n",
      "Training Loss : 0.007886518724262714\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 186.978515625\n",
      "Eval_StdReturn : 6.335563659667969\n",
      "Eval_MaxReturn : 193.31407165527344\n",
      "Eval_MinReturn : 180.6429443359375\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 165.71156311035156\n",
      "Train_StdReturn : 30.366361618041992\n",
      "Train_MaxReturn : 225.861083984375\n",
      "Train_MinReturn : 9.805850982666016\n",
      "Train_AverageEpLen : 199.29850746268656\n",
      "Train_EnvstepsSoFar : 3165118\n",
      "TimeSinceStart : 5634.761869192123\n",
      "Training Loss : 0.00250991340726614\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 180.15316772460938\n",
      "Eval_StdReturn : 15.583641052246094\n",
      "Eval_MaxReturn : 195.73681640625\n",
      "Eval_MinReturn : 164.5695343017578\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 169.2301025390625\n",
      "Train_StdReturn : 29.673463821411133\n",
      "Train_MaxReturn : 222.92239379882812\n",
      "Train_MinReturn : 12.484119415283203\n",
      "Train_AverageEpLen : 199.0497512437811\n",
      "Train_EnvstepsSoFar : 3205127\n",
      "TimeSinceStart : 5706.959115982056\n",
      "Training Loss : -0.004127242136746645\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 181.9722900390625\n",
      "Eval_StdReturn : 30.411285400390625\n",
      "Eval_MaxReturn : 212.38357543945312\n",
      "Eval_MinReturn : 151.56100463867188\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 167.05221557617188\n",
      "Train_StdReturn : 30.509653091430664\n",
      "Train_MaxReturn : 225.58135986328125\n",
      "Train_MinReturn : 45.95453643798828\n",
      "Train_AverageEpLen : 198.73762376237624\n",
      "Train_EnvstepsSoFar : 3245272\n",
      "TimeSinceStart : 5777.740400314331\n",
      "Training Loss : -0.0027368345763534307\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 177.5367431640625\n",
      "Eval_StdReturn : 15.941383361816406\n",
      "Eval_MaxReturn : 193.47811889648438\n",
      "Eval_MinReturn : 161.59535217285156\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 167.3507843017578\n",
      "Train_StdReturn : 31.807449340820312\n",
      "Train_MaxReturn : 218.12164306640625\n",
      "Train_MinReturn : 18.385040283203125\n",
      "Train_AverageEpLen : 198.9950495049505\n",
      "Train_EnvstepsSoFar : 3285469\n",
      "TimeSinceStart : 5849.433887243271\n",
      "Training Loss : -0.0043369801715016365\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 175.72174072265625\n",
      "Eval_StdReturn : 11.514427185058594\n",
      "Eval_MaxReturn : 187.2361602783203\n",
      "Eval_MinReturn : 164.20730590820312\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 166.6226043701172\n",
      "Train_StdReturn : 24.144102096557617\n",
      "Train_MaxReturn : 218.84938049316406\n",
      "Train_MinReturn : 37.96492004394531\n",
      "Train_AverageEpLen : 199.45273631840797\n",
      "Train_EnvstepsSoFar : 3325559\n",
      "TimeSinceStart : 5920.818126440048\n",
      "Training Loss : -0.0020067652221769094\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 180.02346801757812\n",
      "Eval_StdReturn : 9.342567443847656\n",
      "Eval_MaxReturn : 189.3660430908203\n",
      "Eval_MinReturn : 170.680908203125\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 170.52175903320312\n",
      "Train_StdReturn : 21.43429946899414\n",
      "Train_MaxReturn : 212.98153686523438\n",
      "Train_MinReturn : 85.87248229980469\n",
      "Train_AverageEpLen : 199.86069651741295\n",
      "Train_EnvstepsSoFar : 3365731\n",
      "TimeSinceStart : 5991.57400894165\n",
      "Training Loss : 0.0038057249039411545\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 161.93399047851562\n",
      "Eval_StdReturn : 21.62835693359375\n",
      "Eval_MaxReturn : 183.56234741210938\n",
      "Eval_MinReturn : 140.30563354492188\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 165.16024780273438\n",
      "Train_StdReturn : 22.86030387878418\n",
      "Train_MaxReturn : 211.420654296875\n",
      "Train_MinReturn : 105.82083129882812\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 3405731\n",
      "TimeSinceStart : 6062.694292068481\n",
      "Training Loss : -0.0030415344517678022\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 135.98028564453125\n",
      "Eval_StdReturn : 22.97502899169922\n",
      "Eval_MaxReturn : 158.95530700683594\n",
      "Eval_MinReturn : 113.0052490234375\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 164.6587371826172\n",
      "Train_StdReturn : 23.56914520263672\n",
      "Train_MaxReturn : 210.89437866210938\n",
      "Train_MinReturn : 78.90926361083984\n",
      "Train_AverageEpLen : 199.80597014925374\n",
      "Train_EnvstepsSoFar : 3445892\n",
      "TimeSinceStart : 6134.397584676743\n",
      "Training Loss : 0.005602141842246056\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 134.81478881835938\n",
      "Eval_StdReturn : 19.41800308227539\n",
      "Eval_MaxReturn : 154.2327880859375\n",
      "Eval_MinReturn : 115.39678192138672\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 162.28370666503906\n",
      "Train_StdReturn : 22.176197052001953\n",
      "Train_MaxReturn : 213.22805786132812\n",
      "Train_MinReturn : 88.77235412597656\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 3485892\n",
      "TimeSinceStart : 6206.407618999481\n",
      "Training Loss : -0.0037978002801537514\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 167.26768493652344\n",
      "Eval_StdReturn : 3.46136474609375\n",
      "Eval_MaxReturn : 170.7290496826172\n",
      "Eval_MinReturn : 163.8063201904297\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 160.12850952148438\n",
      "Train_StdReturn : 24.414974212646484\n",
      "Train_MaxReturn : 212.25241088867188\n",
      "Train_MinReturn : 34.46474075317383\n",
      "Train_AverageEpLen : 199.93034825870646\n",
      "Train_EnvstepsSoFar : 3526078\n",
      "TimeSinceStart : 6278.284335613251\n",
      "Training Loss : -0.009235004894435406\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 168.4667510986328\n",
      "Eval_StdReturn : 15.639724731445312\n",
      "Eval_MaxReturn : 184.10647583007812\n",
      "Eval_MinReturn : 152.8270263671875\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 158.91903686523438\n",
      "Train_StdReturn : 22.36361312866211\n",
      "Train_MaxReturn : 210.58184814453125\n",
      "Train_MinReturn : 105.16064453125\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 3566078\n",
      "TimeSinceStart : 6350.236792087555\n",
      "Training Loss : 0.001166625996120274\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 181.85122680664062\n",
      "Eval_StdReturn : 8.178703308105469\n",
      "Eval_MaxReturn : 190.02993774414062\n",
      "Eval_MinReturn : 173.6725311279297\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 163.15469360351562\n",
      "Train_StdReturn : 24.161890029907227\n",
      "Train_MaxReturn : 211.8191680908203\n",
      "Train_MinReturn : 36.447818756103516\n",
      "Train_AverageEpLen : 199.8407960199005\n",
      "Train_EnvstepsSoFar : 3606246\n",
      "TimeSinceStart : 6421.848454236984\n",
      "Training Loss : -0.0003189211420249194\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 163.6128692626953\n",
      "Eval_StdReturn : 8.434906005859375\n",
      "Eval_MaxReturn : 172.0477752685547\n",
      "Eval_MinReturn : 155.17796325683594\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 163.80328369140625\n",
      "Train_StdReturn : 23.33473777770996\n",
      "Train_MaxReturn : 211.34910583496094\n",
      "Train_MinReturn : 63.6633415222168\n",
      "Train_AverageEpLen : 199.46766169154228\n",
      "Train_EnvstepsSoFar : 3646339\n",
      "TimeSinceStart : 6492.996882915497\n",
      "Training Loss : 0.0053095510229468346\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 161.62139892578125\n",
      "Eval_StdReturn : 0.6881256103515625\n",
      "Eval_MaxReturn : 162.3095245361328\n",
      "Eval_MinReturn : 160.9332733154297\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 160.44557189941406\n",
      "Train_StdReturn : 26.35804557800293\n",
      "Train_MaxReturn : 205.37464904785156\n",
      "Train_MinReturn : 45.41034698486328\n",
      "Train_AverageEpLen : 199.39800995024876\n",
      "Train_EnvstepsSoFar : 3686418\n",
      "TimeSinceStart : 6564.834169626236\n",
      "Training Loss : 0.006594024132937193\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 151.53768920898438\n",
      "Eval_StdReturn : 11.960868835449219\n",
      "Eval_MaxReturn : 163.49855041503906\n",
      "Eval_MinReturn : 139.57681274414062\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 159.85104370117188\n",
      "Train_StdReturn : 31.436729431152344\n",
      "Train_MaxReturn : 209.61607360839844\n",
      "Train_MinReturn : 8.262279510498047\n",
      "Train_AverageEpLen : 198.2970297029703\n",
      "Train_EnvstepsSoFar : 3726474\n",
      "TimeSinceStart : 6636.2403881549835\n",
      "Training Loss : -0.0033885445445775986\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 171.05325317382812\n",
      "Eval_StdReturn : 7.891120910644531\n",
      "Eval_MaxReturn : 178.9443817138672\n",
      "Eval_MinReturn : 163.16213989257812\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 152.9466094970703\n",
      "Train_StdReturn : 39.21049880981445\n",
      "Train_MaxReturn : 213.80123901367188\n",
      "Train_MinReturn : 20.89153480529785\n",
      "Train_AverageEpLen : 194.06280193236714\n",
      "Train_EnvstepsSoFar : 3766645\n",
      "TimeSinceStart : 6707.526544332504\n",
      "Training Loss : -0.0007059298804961145\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 122.87698364257812\n",
      "Eval_StdReturn : 67.46006774902344\n",
      "Eval_MaxReturn : 190.42315673828125\n",
      "Eval_MinReturn : 30.756431579589844\n",
      "Eval_AverageEpLen : 184.0\n",
      "Train_AverageReturn : 147.0705108642578\n",
      "Train_StdReturn : 46.846439361572266\n",
      "Train_MaxReturn : 208.5709228515625\n",
      "Train_MinReturn : 27.503368377685547\n",
      "Train_AverageEpLen : 191.87081339712918\n",
      "Train_EnvstepsSoFar : 3806746\n",
      "TimeSinceStart : 6779.08811712265\n",
      "Training Loss : 0.0016468167304992676\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 177.14173889160156\n",
      "Eval_StdReturn : 0.1553497314453125\n",
      "Eval_MaxReturn : 177.29708862304688\n",
      "Eval_MinReturn : 176.98638916015625\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 157.28631591796875\n",
      "Train_StdReturn : 37.10621643066406\n",
      "Train_MaxReturn : 209.9075927734375\n",
      "Train_MinReturn : 30.26235580444336\n",
      "Train_AverageEpLen : 194.8398058252427\n",
      "Train_EnvstepsSoFar : 3846883\n",
      "TimeSinceStart : 6849.336535453796\n",
      "Training Loss : -0.007011024281382561\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 148.6968994140625\n",
      "Eval_StdReturn : 14.742630004882812\n",
      "Eval_MaxReturn : 163.4395294189453\n",
      "Eval_MinReturn : 133.9542694091797\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 166.76339721679688\n",
      "Train_StdReturn : 31.74495506286621\n",
      "Train_MaxReturn : 220.13265991210938\n",
      "Train_MinReturn : 17.18667221069336\n",
      "Train_AverageEpLen : 198.08415841584159\n",
      "Train_EnvstepsSoFar : 3886896\n",
      "TimeSinceStart : 6919.838553190231\n",
      "Training Loss : 0.002963897306472063\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 154.76486206054688\n",
      "Eval_StdReturn : 11.211326599121094\n",
      "Eval_MaxReturn : 165.9761962890625\n",
      "Eval_MinReturn : 143.5535430908203\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 163.76698303222656\n",
      "Train_StdReturn : 30.525236129760742\n",
      "Train_MaxReturn : 212.74569702148438\n",
      "Train_MinReturn : 35.11321258544922\n",
      "Train_AverageEpLen : 198.65841584158414\n",
      "Train_EnvstepsSoFar : 3927025\n",
      "TimeSinceStart : 6990.705791711807\n",
      "Training Loss : 0.0043272641487419605\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 141.92141723632812\n",
      "Eval_StdReturn : 20.97740936279297\n",
      "Eval_MaxReturn : 162.89883422851562\n",
      "Eval_MinReturn : 120.94401550292969\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 166.80801391601562\n",
      "Train_StdReturn : 26.5849609375\n",
      "Train_MaxReturn : 213.67042541503906\n",
      "Train_MinReturn : 16.44986343383789\n",
      "Train_AverageEpLen : 199.955223880597\n",
      "Train_EnvstepsSoFar : 3967216\n",
      "TimeSinceStart : 7061.971116542816\n",
      "Training Loss : 0.0023377235047519207\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 186.9075927734375\n",
      "Eval_StdReturn : 10.009765625\n",
      "Eval_MaxReturn : 196.9173583984375\n",
      "Eval_MinReturn : 176.8978271484375\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 155.42494201660156\n",
      "Train_StdReturn : 29.211971282958984\n",
      "Train_MaxReturn : 210.07168579101562\n",
      "Train_MinReturn : 27.82757568359375\n",
      "Train_AverageEpLen : 199.9402985074627\n",
      "Train_EnvstepsSoFar : 4007404\n",
      "TimeSinceStart : 7133.389338970184\n",
      "Training Loss : -0.002236312488093972\n",
      "Initial_DataCollection_AverageReturn : -325.2880554199219\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## run training\n",
    "\n",
    "print(args.logdir)\n",
    "trainer = PG_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "km7LlYvhqKTl"
   },
   "outputs": [],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "## requires tensorflow==2.3.0\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir D:/Code/RL-homework/hw2/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('rlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e99c6a051cabfccea28eda58226a3deb0006c283efe5463cd051c492fa4b03c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
