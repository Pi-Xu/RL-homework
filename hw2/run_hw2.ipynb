{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qUmV93fif6S"
   },
   "source": [
    "## Run Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "lN-gZkqiijnR"
   },
   "outputs": [],
   "source": [
    "#@title imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
    "from cs285.agents.pg_agent import PGAgent\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "Q6NaOWhOinnU"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  def __contains__(self, key):\n",
    "    return hasattr(self, key)\n",
    "\n",
    "  env_name = 'CartPole-v0' #@param\n",
    "  exp_name = 'q1_sb_rtg_na' #@param\n",
    "\n",
    "  #@markdown main parameters of interest\n",
    "  n_iter = 100 #@param {type: \"integer\"}\n",
    "\n",
    "  ## PDF will tell you how to set ep_len\n",
    "  ## and discount for each environment\n",
    "  ep_len = 200 #@param {type: \"integer\"}\n",
    "  discount = 0.95 #@param {type: \"number\"}\n",
    "\n",
    "  reward_to_go = False #@param {type: \"boolean\"}\n",
    "  nn_baseline = False #@param {type: \"boolean\"}\n",
    "  gae_lambda = None #@param {type: \"number\"}\n",
    "  dont_standardize_advantages = False #@param {type: \"boolean\"}\n",
    "\n",
    "  #@markdown batches and steps\n",
    "  batch_size = 1000 #@param {type: \"integer\"}\n",
    "  eval_batch_size = 400 #@param {type: \"integer\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1 #@param {type: \"integer\"}\n",
    "  learning_rate =  5e-3 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown MLP parameters\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown system\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "    \n",
    "  action_noise_std = 0 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  ## default is to not log video so\n",
    "  ## that logs are small enough to be\n",
    "  ## uploaded to gradscope\n",
    "  video_log_freq =  -1#@param {type: \"integer\"}\n",
    "  scalar_log_freq =  1#@param {type: \"integer\"}\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "## ensure compatibility with hw1 code\n",
    "args['train_batch_size'] = args['batch_size']\n",
    "\n",
    "if args['video_log_freq'] > 0:\n",
    "  import warnings\n",
    "  warnings.warn(\n",
    "      '''\\nLogging videos will make eventfiles too'''\n",
    "      '''\\nlarge for the autograder. Set video_log_freq = -1'''\n",
    "      '''\\nfor the runs you intend to submit.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "eScWwHhnsYkd"
   },
   "outputs": [],
   "source": [
    "#@title create directory for logging\n",
    "\n",
    "data_path =r'D:\\Code\\RL-homework\\hw2\\data'\n",
    "\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "logdir = args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aljzrLdAsvNu"
   },
   "outputs": [],
   "source": [
    "## define policy gradient trainer\n",
    "\n",
    "class PG_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "\n",
    "        #####################\n",
    "        ## SET AGENT PARAMS\n",
    "        #####################\n",
    "\n",
    "        computation_graph_args = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            }\n",
    "\n",
    "        estimate_advantage_args = {\n",
    "            'gamma': params['discount'],\n",
    "            'standardize_advantages': not(params['dont_standardize_advantages']),\n",
    "            'reward_to_go': params['reward_to_go'],\n",
    "            'nn_baseline': params['nn_baseline'],\n",
    "            'gae_lambda': params['gae_lambda'],\n",
    "        }\n",
    "\n",
    "        train_args = {\n",
    "            'num_agent_train_steps_per_iter': params['num_agent_train_steps_per_iter'],\n",
    "        }\n",
    "\n",
    "        agent_params = {**computation_graph_args, **estimate_advantage_args, **train_args}\n",
    "\n",
    "        self.params = params\n",
    "        self.params['agent_class'] = PGAgent\n",
    "        self.params['agent_params'] = agent_params\n",
    "        self.params['batch_size_initial'] = self.params['batch_size']\n",
    "\n",
    "        ################\n",
    "        ## RL TRAINER\n",
    "        ################\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params)\n",
    "\n",
    "    def run_training_loop(self):\n",
    "\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            self.params['n_iter'],\n",
    "            collect_policy = self.rl_trainer.agent.actor,\n",
    "            eval_policy = self.rl_trainer.agent.actor,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2rCuQsRsd3N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\RL-homework\\hw2\\data\\q1_sb_rtg_na_CartPole-v0_06-09-2022_13-23-09\n",
      "########################\n",
      "logging outputs to  D:\\Code\\RL-homework\\hw2\\data\\q1_sb_rtg_na_CartPole-v0_06-09-2022_13-23-09\n",
      "########################\n",
      "Using GPU id 0\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 27.0\n",
      "Eval_StdReturn : 12.88927173614502\n",
      "Eval_MaxReturn : 54.0\n",
      "Eval_MinReturn : 12.0\n",
      "Eval_AverageEpLen : 27.0\n",
      "Train_AverageReturn : 27.648649215698242\n",
      "Train_StdReturn : 12.643160820007324\n",
      "Train_MaxReturn : 65.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 27.64864864864865\n",
      "Train_EnvstepsSoFar : 1023\n",
      "TimeSinceStart : 2.714303731918335\n",
      "Training Loss : -0.003919023554772139\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 34.58333206176758\n",
      "Eval_StdReturn : 22.691621780395508\n",
      "Eval_MaxReturn : 98.0\n",
      "Eval_MinReturn : 11.0\n",
      "Eval_AverageEpLen : 34.583333333333336\n",
      "Train_AverageReturn : 26.0\n",
      "Train_StdReturn : 14.586610794067383\n",
      "Train_MaxReturn : 66.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 26.0\n",
      "Train_EnvstepsSoFar : 2037\n",
      "TimeSinceStart : 3.977308988571167\n",
      "Training Loss : 0.005789920687675476\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 40.400001525878906\n",
      "Eval_StdReturn : 13.146862983703613\n",
      "Eval_MaxReturn : 68.0\n",
      "Eval_MinReturn : 21.0\n",
      "Eval_AverageEpLen : 40.4\n",
      "Train_AverageReturn : 36.5\n",
      "Train_StdReturn : 17.29677963256836\n",
      "Train_MaxReturn : 76.0\n",
      "Train_MinReturn : 12.0\n",
      "Train_AverageEpLen : 36.5\n",
      "Train_EnvstepsSoFar : 3059\n",
      "TimeSinceStart : 5.075455188751221\n",
      "Training Loss : -0.010536029934883118\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 38.75\n",
      "Eval_StdReturn : 22.390939712524414\n",
      "Eval_MaxReturn : 87.0\n",
      "Eval_MinReturn : 16.0\n",
      "Eval_AverageEpLen : 38.75\n",
      "Train_AverageReturn : 38.61538314819336\n",
      "Train_StdReturn : 11.08386516571045\n",
      "Train_MaxReturn : 54.0\n",
      "Train_MinReturn : 18.0\n",
      "Train_AverageEpLen : 38.61538461538461\n",
      "Train_EnvstepsSoFar : 4063\n",
      "TimeSinceStart : 6.35428524017334\n",
      "Training Loss : -0.0026139928959310055\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 53.125\n",
      "Eval_StdReturn : 23.449081420898438\n",
      "Eval_MaxReturn : 94.0\n",
      "Eval_MinReturn : 29.0\n",
      "Eval_AverageEpLen : 53.125\n",
      "Train_AverageReturn : 53.105262756347656\n",
      "Train_StdReturn : 27.686010360717773\n",
      "Train_MaxReturn : 116.0\n",
      "Train_MinReturn : 19.0\n",
      "Train_AverageEpLen : 53.10526315789474\n",
      "Train_EnvstepsSoFar : 5072\n",
      "TimeSinceStart : 7.452033519744873\n",
      "Training Loss : -0.010685451328754425\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 41.400001525878906\n",
      "Eval_StdReturn : 15.60897159576416\n",
      "Eval_MaxReturn : 69.0\n",
      "Eval_MinReturn : 16.0\n",
      "Eval_AverageEpLen : 41.4\n",
      "Train_AverageReturn : 50.761905670166016\n",
      "Train_StdReturn : 32.306427001953125\n",
      "Train_MaxReturn : 153.0\n",
      "Train_MinReturn : 18.0\n",
      "Train_AverageEpLen : 50.76190476190476\n",
      "Train_EnvstepsSoFar : 6138\n",
      "TimeSinceStart : 8.900340557098389\n",
      "Training Loss : -0.004316273145377636\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 66.66666412353516\n",
      "Eval_StdReturn : 21.25767707824707\n",
      "Eval_MaxReturn : 102.0\n",
      "Eval_MinReturn : 36.0\n",
      "Eval_AverageEpLen : 66.66666666666667\n",
      "Train_AverageReturn : 46.04545593261719\n",
      "Train_StdReturn : 19.87798500061035\n",
      "Train_MaxReturn : 107.0\n",
      "Train_MinReturn : 22.0\n",
      "Train_AverageEpLen : 46.04545454545455\n",
      "Train_EnvstepsSoFar : 7151\n",
      "TimeSinceStart : 10.083835124969482\n",
      "Training Loss : -0.005947170779109001\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 63.14285659790039\n",
      "Eval_StdReturn : 29.29860496520996\n",
      "Eval_MaxReturn : 130.0\n",
      "Eval_MinReturn : 38.0\n",
      "Eval_AverageEpLen : 63.142857142857146\n",
      "Train_AverageReturn : 77.92857360839844\n",
      "Train_StdReturn : 34.87623977661133\n",
      "Train_MaxReturn : 159.0\n",
      "Train_MinReturn : 30.0\n",
      "Train_AverageEpLen : 77.92857142857143\n",
      "Train_EnvstepsSoFar : 8242\n",
      "TimeSinceStart : 11.443286180496216\n",
      "Training Loss : -0.005109989549964666\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 69.83333587646484\n",
      "Eval_StdReturn : 33.908782958984375\n",
      "Eval_MaxReturn : 140.0\n",
      "Eval_MinReturn : 33.0\n",
      "Eval_AverageEpLen : 69.83333333333333\n",
      "Train_AverageReturn : 71.92857360839844\n",
      "Train_StdReturn : 43.84789276123047\n",
      "Train_MaxReturn : 194.0\n",
      "Train_MinReturn : 19.0\n",
      "Train_AverageEpLen : 71.92857142857143\n",
      "Train_EnvstepsSoFar : 9249\n",
      "TimeSinceStart : 12.733090162277222\n",
      "Training Loss : -0.005650873761624098\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n"
     ]
    }
   ],
   "source": [
    "## run training\n",
    "\n",
    "print(args.logdir)\n",
    "trainer = PG_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "km7LlYvhqKTl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 19592), started 0:05:24 ago. (Use '!kill 19592' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-61764a69c4c3a117\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-61764a69c4c3a117\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "\n",
    "## requires tensorflow==2.3.0\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir D:/Code/RL-homework/hw2/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('rlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e99c6a051cabfccea28eda58226a3deb0006c283efe5463cd051c492fa4b03c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
