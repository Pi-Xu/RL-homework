{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQx7oDGeeKWj"
   },
   "source": [
    "## Editing Code\n",
    "\n",
    "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`cs285_f2021/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UunygyDXrx7k"
   },
   "source": [
    "## Run Behavior Cloning (Problem 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "enh5ZMHftEO7"
   },
   "outputs": [],
   "source": [
    "#@title imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from cs285.infrastructure.rl_trainer import RL_Trainer\n",
    "from cs285.agents.bc_agent import BCAgent\n",
    "from cs285.policies.loaded_gaussian_policy import LoadedGaussianPolicy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "imnAkQ6jryL7"
   },
   "outputs": [],
   "source": [
    "#@title runtime arguments\n",
    "\n",
    "class Args:\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    return getattr(self, key)\n",
    "\n",
    "  def __setitem__(self, key, val):\n",
    "    setattr(self, key, val)\n",
    "\n",
    "  #@markdown expert data\n",
    "  expert_policy_file = 'cs285/policies/experts/Ant.pkl' #@param\n",
    "  expert_data = r'cs285\\expert_data\\expert_data_Ant-v2.pkl' #@param\n",
    "  env_name = 'Ant-v2' #@param ['Ant-v2', 'Humanoid-v2', 'Walker2d-v2', 'HalfCheetah-v2', 'Hopper-v2']\n",
    "  exp_name = 'test_bc_Ant' #@param\n",
    "  do_dagger = True #@param {type: \"boolean\"}\n",
    "  ep_len = 1000 #@param {type: \"integer\"}# each of these collected rollouts to be of length self.params['ep_len']\n",
    "  save_params = False #@param {type: \"boolean\"}\n",
    "\n",
    "  num_agent_train_steps_per_iter = 1000 #@param {type: \"integer\"}) # 进行多少次训练(train_step)\n",
    "  n_iter = 10 #@param {type: \"integer\"})\n",
    "\n",
    "  #@markdown batches & buffers\n",
    "  batch_size = 1000 #@param {type: \"integer\"}) # collect_training_trajectories里面使用, collect多少\n",
    "  eval_batch_size = 1000 #@param {type: \"integer\"} # 在perform_logging里面eval的时候\n",
    "  train_batch_size = 100 #@param {type: \"integer\"} # 在train过程中从replay_buffer sample出来的四元组大小\n",
    "  max_replay_buffer_size = 1000000 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown network\n",
    "  n_layers = 2 #@param {type: \"integer\"}\n",
    "  size = 64 #@param {type: \"integer\"}\n",
    "  learning_rate = 5e-3 #@param {type: \"number\"}\n",
    "\n",
    "  #@markdown logging\n",
    "  video_log_freq = -1 #@param {type: \"integer\"}\n",
    "  scalar_log_freq = 1 #@param {type: \"integer\"}\n",
    "\n",
    "  #@markdown gpu & run-time settings\n",
    "  no_gpu = False #@param {type: \"boolean\"}\n",
    "  which_gpu = 0 #@param {type: \"integer\"}\n",
    "  seed = 1 #@param {type: \"integer\"}\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "fLnU1evmss4I"
   },
   "outputs": [],
   "source": [
    "#@title define `BC_Trainer`\n",
    "class BC_Trainer(object):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        #######################\n",
    "        ## AGENT PARAMS\n",
    "        #######################\n",
    "\n",
    "        agent_params = {\n",
    "            'n_layers': params['n_layers'],\n",
    "            'size': params['size'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'max_replay_buffer_size': params['max_replay_buffer_size'],\n",
    "            }\n",
    "\n",
    "        self.params = params\n",
    "        self.params['agent_class'] = BCAgent ## TODO: look in here and implement this\n",
    "        self.params['agent_params'] = agent_params\n",
    "\n",
    "        ################\n",
    "        ## RL TRAINER\n",
    "        ################\n",
    "\n",
    "        self.rl_trainer = RL_Trainer(self.params) ## TODO: look in here and implement this\n",
    "\n",
    "        #######################\n",
    "        ## LOAD EXPERT POLICY\n",
    "        #######################\n",
    "\n",
    "        print('Loading expert policy from...', self.params['expert_policy_file'])\n",
    "        self.loaded_expert_policy = LoadedGaussianPolicy(self.params['expert_policy_file'])\n",
    "        print('Done restoring expert policy...')\n",
    "\n",
    "    def run_training_loop(self):\n",
    "\n",
    "        self.rl_trainer.run_training_loop(\n",
    "            n_iter=self.params['n_iter'],\n",
    "            initial_expertdata=self.params['expert_data'],\n",
    "            collect_policy=self.rl_trainer.agent.actor,\n",
    "            eval_policy=self.rl_trainer.agent.actor,\n",
    "            relabel_with_expert=self.params['do_dagger'],\n",
    "            expert_policy=self.loaded_expert_policy,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "7UkzHBfxsxH8"
   },
   "outputs": [],
   "source": [
    "#@title create directory for logging\n",
    "\n",
    "if args.do_dagger:\n",
    "    logdir_prefix = 'q2_'  # The autograder uses the prefix `q2_`\n",
    "    assert args.n_iter>1, ('DAgger needs more than 1 iteration (n_iter>1) of training, to iteratively query the expert and train (after 1st warmstarting from behavior cloning).')\n",
    "else:\n",
    "    logdir_prefix = 'q1_'  # The autograder uses the prefix `q1_`\n",
    "    assert args.n_iter==1, ('Vanilla behavior cloning collects expert data just once (n_iter=1)')\n",
    "\n",
    "data_path =r'D:\\Code\\RL-homework\\hw1\\data'\n",
    "if not (os.path.exists(data_path)):\n",
    "    os.makedirs(data_path)\n",
    "logdir = logdir_prefix + args.exp_name + '_' + args.env_name + \\\n",
    "         '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not(os.path.exists(logdir)):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qQb789_syt0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Code\\RL-homework\\hw1\\data\\q2_test_bc_Ant_Ant-v2_14-08-2022_23-10-10\n",
      "########################\n",
      "logging outputs to  D:\\Code\\RL-homework\\hw1\\data\\q2_test_bc_Ant_Ant-v2_14-08-2022_23-10-10\n",
      "########################\n",
      "Using GPU id 0\n",
      "Loading expert policy from... cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2603.708740234375\n",
      "Eval_StdReturn : 1946.865966796875\n",
      "Eval_MaxReturn : 4550.57470703125\n",
      "Eval_MinReturn : 656.8426513671875\n",
      "Eval_AverageEpLen : 576.0\n",
      "Train_AverageReturn : 4713.6533203125\n",
      "Train_StdReturn : 12.196533203125\n",
      "Train_MaxReturn : 4725.849609375\n",
      "Train_MinReturn : 4701.45654296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 63.984598875045776\n",
      "Training Loss : 0.001794147421605885\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n"
     ]
    }
   ],
   "source": [
    "## run training\n",
    "print(args.logdir)\n",
    "trainer = BC_Trainer(args)\n",
    "trainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75M0MlR5tUIb"
   },
   "outputs": [],
   "source": [
    "#@markdown You can visualize your runs with tensorboard from within the notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='D:\\Code\\RL-homework\\hw1\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ff9onuUPfPEa"
   },
   "source": [
    "## Running DAgger (Problem 2)\n",
    "Modify the settings above:\n",
    "1. check the `do_dagger` box\n",
    "2. set `n_iters` to `10`\n",
    "and then rerun the code."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "run_hw1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('rlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e99c6a051cabfccea28eda58226a3deb0006c283efe5463cd051c492fa4b03c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
